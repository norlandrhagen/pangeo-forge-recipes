{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49caf2b2",
   "metadata": {},
   "source": [
    "# GRIB2 Reference Recipe for HRRR (High-Resolution Rapid Refresh)\n",
    "\n",
    "This notebook examples uses the {class}`pangeo_forge_recipes.recipes.ReferenceRecipe` to create a reference index of the HRRR dataset. Since it is a kerchunk based reference recipe, none of the source data files are transfered, only the `.json` kerchunk index is copied over. \n",
    "\n",
    "For more background, see [this blog post](https://medium.com/pangeo/fake-it-until-you-make-it-reading-goes-netcdf4-data-on-aws-s3-as-zarr-for-rapid-data-access-61e33f8fe685).\n",
    "\n",
    "The HRRR dataset is an atmospheric model produced by NOAA in near-real-time. The output model data are stored in the GRIB2 format, which is a common format in weather forecasting and modeling. By using the Kerchunk-based {class}`pangeo_forge_recipes.recipes.ReferenceRecipe`, we can read this dataset as if it were `Zarr`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f9855",
   "metadata": {},
   "source": [
    "## Define the FilePattern\n",
    "\n",
    "Here we will select some files from the HRRR data archive on the [AWS open data registry](https://registry.opendata.aws/noaa-hrrr-pds/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965272cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fsspec \n",
    "\n",
    "# Initiate fsspec filesystems for reading and writing\n",
    "fs = fsspec.filesystem('s3', anon=True, skip_instance_cache=True)\n",
    "\n",
    "# retrieve list of available days in archive\n",
    "days_available = fs.glob('s3://noaa-hrrr-bdp-pds/hrrr.*')\n",
    "\n",
    "# Read HRRR GRIB2 files from latest day\n",
    "files = fs.glob(f's3://{days_available[-1]}/conus/*wrfsfcf01.grib2')\n",
    "\n",
    "# Append s3 prefix for filelist\n",
    "files = sorted(['s3://'+f for f in files])\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9cadd",
   "metadata": {},
   "source": [
    "Examine one of the files with xarray.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "\n",
    "ex_file = fsspec.open_local(\"simplecache::\"+files[0], s3={'anon': True}, filecache={'cache_storage':'/tmp/files'})\n",
    "ds = xr.open_dataset(ex_file, engine=\"cfgrib\", filter_by_keys={'stepType': 'instant','typeOfLevel': 'heightAboveGround'})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de47e7f-5196-4b98-b05d-f281cb2eb056",
   "metadata": {},
   "source": [
    "Opening up a single grib file took over 1.5 minutes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5d0a3-fdee-4072-a621-b905427cd616",
   "metadata": {},
   "source": [
    "## Define the Recipe\n",
    "\n",
    "\n",
    "As a first step in our recipe, we create a `File Pattern <../../recipe_user_guide/file_patterns>` to represent the input files.\n",
    "In this case, since we already have a list of inputs, we just use the `pattern_from_file_sequence` convenience function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47008ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_forge_recipes.patterns import pattern_from_file_sequence\n",
    "pattern = pattern_from_file_sequence(files, 'step', file_type='grib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e257f",
   "metadata": {},
   "source": [
    "In the `GribReferenceRecipe` class we can pass kwargs such as: `output_storage_options` and `grib_filter_by_keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb5785-f8fd-4b2d-b6ef-5377ab6691b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_forge_recipes.recipes import ReferenceRecipe\n",
    "\n",
    "data_filter={'typeOfLevel': 'heightAboveGround', 'level': [2, 10]}    \n",
    "storage_options = {\"anon\": True}\n",
    "\n",
    "recipe = ReferenceRecipe(pattern, storage_options=storage_options,grib_filters=data_filter)\n",
    "\n",
    "recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc8d97e-da09-4fb0-a60f-6443c5a78139",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Storage\n",
    "\n",
    "If the recipe excecution occurs in a Bakery, cloud storage will be assigned automatically.\n",
    "\n",
    "For this example, we use the recipe's default storage, which is a temporary local directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15e8c9-3873-40e2-a9e2-88ffab45ddaf",
   "metadata": {},
   "source": [
    "## Execute recipe\n",
    "\n",
    "For testing, we will use the `copy_pruned()` utility, which will create a subset of the recipe for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_pruned = recipe.copy_pruned()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9f33a",
   "metadata": {},
   "source": [
    "Next we are converting the recipe to a python function for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = recipe_pruned.to_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f5b49-4b3c-4dc5-8519-63485456af94",
   "metadata": {},
   "source": [
    "## Examine the Result\n",
    "\n",
    "### Load with Intake\n",
    "\n",
    "The easiest way to load the dataset created by `fsspec_reference_maker` is via intake.\n",
    "An intake catalog is automatically created in the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d861e0-a63d-47bf-bc24-4eff18174af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_url = f\"{recipe_pruned.target}/reference.yaml\"\n",
    "cat_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db22ede-a6c9-46eb-9266-501e10eb6b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "cat = intake.open_catalog(cat_url)\n",
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883814d-9e07-49e9-ac45-96e193fab7a7",
   "metadata": {},
   "source": [
    "To load the data lazily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d702f7-bd3a-42e3-ae70-ef2e8c4352ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ds = cat.data.to_dask()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2f4db-a19f-4f4f-a173-06e377b70aa4",
   "metadata": {},
   "source": [
    "### Manual Loading\n",
    "\n",
    "It is also possible to load the reference dataset directly with xarray, bypassing intake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f7497-ef4b-41cc-a8ef-bec225c311f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_url = f\"{recipe_pruned.target}/reference.json\"\n",
    "ref_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a53eca-6c19-4b9e-82bb-3c0c188361c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "m = fsspec.get_mapper(\n",
    "    \"reference://\",\n",
    "    fo=ref_url,\n",
    "    target_protocol=\"file\",\n",
    "    remote_protocol=\"s3\",\n",
    "    remote_options=dict(anon=True),\n",
    "    skip_instance_cache=True,\n",
    ")\n",
    "ds = xr.open_dataset(\n",
    "    m,\n",
    "    engine='zarr',\n",
    "    backend_kwargs={'consolidated': False},\n",
    "    chunks={},\n",
    "    decode_coords=\"all\"\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c347d80-a493-4434-afbd-5f3b0321f783",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Make a Map\n",
    "\n",
    "Let's just verify that we can read an visualize the data. We'll compare the first year to the last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95c115-3cff-4454-83fd-8d9b89a77560",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['2t'][-1].plot()"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": false,
   "timeout": 3000
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pangeo-forge-recipes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8472a85c0cbfd90fc84f178c7f47d34e20396f42fa331cce9968659ce876ac9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
